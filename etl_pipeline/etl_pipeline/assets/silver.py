from dagster import asset, AssetIn, Output
import os
from pyspark.sql import DataFrame
from pyspark.sql.functions import col
import polars as pl

from dagster import asset, AssetIn, Output
import os
from pyspark.sql import DataFrame
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit, current_timestamp
from delta.tables import DeltaTable
from pyspark.sql.functions import col, round, concat, md5, lit

from dagster import asset_check, AssetCheckResult


# Import resource
from ..resources.spark_io_manager import get_spark_session
from .etl_job.insert_job_log import insert_job_log

# Import Utility v·ª´a t·∫°o
from .etl_job.spark_utils import (
    get_watermark_from_meta,
    process_dedup_logic,
    calculate_merge_metrics,
    update_watermark_meta,
    get_source_keys_jdbc,  # H√†m ƒë·ªçc ID t·ª´ MySQL
    sync_deleted_records,  # H√†m x·ª≠ l√Ω x√≥a (Anti Join)
)

COMPUTE_KIND = "PySpark"
LAYER = "silver"


META_TABLE = "silver.etl_watermarks"
WATERMARK_COL = "_ingested_at"


# ==============================================================================
# HELPER: L·∫§Y C·∫§U H√åNH K·∫æT N·ªêI MYSQL (CONNECTION ONLY)
# ==============================================================================
def get_base_mysql_config():
    """
    Tr·∫£ v·ªÅ th√¥ng tin k·∫øt n·ªëi Server MySQL t·ª´ bi·∫øn m√¥i tr∆∞·ªùng.
    """
    db_name = os.getenv("OLIST_DB_NAME", "olist")

    return {
        "url": f"jdbc:mysql://{os.getenv('MYSQL_HOST')}:{os.getenv('MYSQL_PORT')}/{db_name}",
        "user": os.getenv("MYSQL_ROOT_USER", "root"),  # admin (theo file .env c·ªßa b·∫°n)
        "password": os.getenv("MYSQL_PASSWORD"),  # admin
        "driver": "com.mysql.cj.jdbc.Driver",
    }


# ==============================================================================
# WRAPPER FUNCTION (LOGIC X·ª¨ L√ù CHUNG CHO C√ÅC B·∫¢NG SILVER)
# ==============================================================================
def _process_silver_asset(
    context,
    df: DataFrame,
    target_table: str,
    asset_key: str,
    merge_key: str,
    transform_func=None,
    mysql_table: str = None,
    mysql_key: str = None,
):
    config = {
        "endpoint_url": os.getenv("MINIO_ENDPOINT"),
        "minio_access_key": os.getenv("MINIO_ACCESS_KEY"),
        "minio_secret_key": os.getenv("MINIO_SECRET_KEY"),
    }
    run_id = str(context.run.run_id).split("-")[0]

    with get_spark_session(config, run_id) as spark:

        insert_job_log(
            spark=spark,
            job_name=f"silver_{asset_key}",
            layer="silver",
            source="staging",
            target_table=target_table,
            merge_key=merge_key,
            load_mode="upsert",
            schedule="daily",
            owner="hung.nguyen",
            description=f"Clean & Upsert {asset_key}",
        )

        # 1. WATERMARK & INCREMENTAL FILTER
        context.log.info(f"üïµÔ∏è Checking watermark in {META_TABLE} for {asset_key}...")
        high_watermark = get_watermark_from_meta(spark, META_TABLE, asset_key)

        deduped_df, new_batch_watermark = process_dedup_logic(
            spark, df, merge_key, WATERMARK_COL, high_watermark
        )

        # 2. TRANSFORM
        if transform_func:
            deduped_df = transform_func(deduped_df)

        # Th√™m c·ªôt is_active m·∫∑c ƒë·ªãnh l√† True cho data m·ªõi/update
        deduped_df = deduped_df.withColumn("is_active", lit(True))

        # 3. METRICS (UPSERT)
        inserts, updates = 0, 0
        if not deduped_df.rdd.isEmpty():
            inserts, updates = calculate_merge_metrics(
                spark, deduped_df, target_table, merge_key
            )
            context.log.info(f"üì¶ Upsert Batch: {deduped_df.count()} rows.")
        else:
            context.log.info("üí§ No new incremental data.")

        # =================================================================
        # 4. SYNC DELETES (AUTO CONFIG T·ª™ MYSQL)
        # =================================================================
        deleted_count = 0
        if mysql_table and mysql_key:
            context.log.info(
                f"üîå Preparing MySQL connection for table '{mysql_table}'..."
            )

            jdbc_config = get_base_mysql_config()
            jdbc_config["table"] = mysql_table
            jdbc_config["key"] = mysql_key

            # --- ƒêI·ªÇM S·ª¨A ƒê·ªîI QUAN TR·ªåNG ---
            # Truy·ªÅn merge_key v√†o ƒë√¢y ƒë·ªÉ h√†m util t·ª± ƒë·ªïi t√™n c·ªôt MySQL th√†nh t√™n c·ªôt Silver
            full_source_keys = get_source_keys_jdbc(
                spark, jdbc_config, target_col_name=merge_key, logger=context.log
            )
            print(full_source_keys.limit(5))

            if full_source_keys:
                context.log.info("üóë Syncing Deletes (Anti Join)...")
                deleted_count = sync_deleted_records(
                    spark, target_table, full_source_keys, merge_key, logger=context.log
                )
            else:
                context.log.warning(
                    f"‚ö†Ô∏è Could not fetch keys from MySQL table {mysql_table}."
                )

        # 5. UPDATE WATERMARK
        if new_batch_watermark:
            update_watermark_meta(spark, META_TABLE, asset_key, new_batch_watermark)

        return Output(
            value=deduped_df,
            metadata={
                "table": target_table,
                "type": "incremental_merge_delete",
                "watermark_used": str(high_watermark),
                "inserts": inserts,
                "updates": updates,
                "soft_deletes": deleted_count,
            },
        )


# ==============================================================================
# ASSET: SILVER CLEANED CUSTOMER
# ==============================================================================
@asset(
    description="Clean & Deduplicate Customers (Sync Delete from MySQL)",
    ins={"silver_stg_customer": AssetIn(key_prefix=["silver", "customer"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "customer"],
    name="silver_cleaned_customer",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "customer_id"},
)
def silver_cleaned_customer(context, silver_stg_customer: DataFrame):

    # Logic transform ri√™ng cho Customer (gi·ªØ nguy√™n logic c≈© c·ªßa b·∫°n)
    def transform_logic(df):
        return df.na.drop()

    return _process_silver_asset(
        context,
        df=silver_stg_customer,
        target_table="silver.clean_customer",
        asset_key="clean_customer",
        merge_key="customer_id",
        transform_func=transform_logic,
        # C·∫•u h√¨nh Sync Delete: Ch·ªâ c·∫ßn t√™n b·∫£ng v√† t√™n c·ªôt Key ·ªü MySQL
        mysql_table="customers",
        mysql_key="customer_id",
    )


@asset(
    description="Clean, Deduplicate & Sync Delete for Sellers",
    ins={
        # L∆∞u √Ω: Input b√¢y gi·ªù l√† Staging (Spark DF) thay v√¨ Bronze (Polars)
        # ƒë·ªÉ t·∫≠n d·ª•ng lu·ªìng Incremental Load
        "silver_stg_seller": AssetIn(key_prefix=["silver", "seller"])
    },
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "seller"],
    name="silver_cleaned_seller",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "seller_id"},
)
def silver_cleaned_seller(context, silver_stg_seller: DataFrame):
    """
    Pipeline x·ª≠ l√Ω b·∫£ng Sellers:
    1. L·∫•y d·ªØ li·ªáu m·ªõi (Incremental).
    2. Clean & Dedup.
    3. ƒê·ªìng b·ªô x√≥a t·ª´ MySQL (Hard Delete -> Soft Delete).
    """

    # Logic Transform ri√™ng cho Seller (n·∫øu c√≥)
    def transform_logic(df):
        # V√≠ d·ª•: X√≥a d√≤ng null, chu·∫©n h√≥a city...
        return df.na.drop()

    return _process_silver_asset(
        context,
        df=silver_stg_seller,
        target_table="silver.clean_seller",
        asset_key="clean_seller",
        merge_key="seller_id",
        transform_func=transform_logic,
        # --- C·∫§U H√åNH SYNC DELETE ---
        mysql_table="sellers",  # T√™n b·∫£ng g·ªëc trong MySQL
        mysql_key="seller_id",  # T√™n c·ªôt ID trong MySQL
        logger=context.log,  # Logger ƒë·ªÉ hi·ªán UI ƒë·∫πp
    )


@asset_check(
    asset=silver_cleaned_customer, description="Check data quality: ID must not be Null"
)
def check_customer_id_not_null(context):
    # 1. L·∫•y Config (Gi·∫£ s·ª≠ b·∫°n load t·ª´ bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c file config chung)
    # N·∫øu b·∫°n d√πng resource config ph·ª©c t·∫°p, c·∫ßn extract t·ª´ context.
    # ·ªû ƒë√¢y m√¨nh v√≠ d·ª• config ƒë∆°n gi·∫£n ƒë·ªÉ init spark.
    spark_config = {
        "endpoint_url": "minio:9000",
        "minio_access_key": "admin",
        "minio_secret_key": "password",
        # ... c√°c config kh√°c
    }

    # T·∫°o Run ID gi·∫£ l·∫≠p cho session check
    check_run_id = f"check_cust_{context.run_id[:8]}"

    # 2. M·ªü Spark Session M·ªöI (Session c·ªßa Asset c≈© ƒë√£ ƒë√≥ng r·ªìi n√™n OK)
    with get_spark_session(spark_config, run_id=check_run_id) as spark:

        # 3. ƒê·ªçc d·ªØ li·ªáu: D√πng spark.table ƒë·ªÉ ch·∫Øc ch·∫Øn ƒë·ªçc ƒë√∫ng b·∫£ng v·ª´a ghi
        # Thay v√¨ .load("path"), ta d√πng .table("t√™n_b·∫£ng")
        df = spark.table("silver.clean_customer")

        # 4. Logic ki·ªÉm tra
        total_rows = df.count()
        null_count = df.filter("customer_id IS NULL").count()

        # Log ra UI cho d·ªÖ nh√¨n
        context.log.info(f"üîç Checked {total_rows} rows. Found {null_count} null IDs.")

        # 5. Tr·∫£ v·ªÅ k·∫øt qu·∫£
        return AssetCheckResult(
            passed=(null_count == 0),
            metadata={
                "null_row_count": null_count,
                "total_rows": total_rows,
                "target_table": "silver.clean_customer",
            },
        )


# ==============================================================================
# 3. PRODUCT ASSET (Updated)
# ==============================================================================
@asset(
    description="Clean, Cast Types & Sync Delete for Products",
    ins={"silver_stg_product": AssetIn(key_prefix=["silver", "product"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "product"],
    name="silver_cleaned_product",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "product_id"},
)
def silver_cleaned_product(context, silver_stg_product: DataFrame):
    """
    Pipeline x·ª≠ l√Ω b·∫£ng Products:
    1. Cast c√°c c·ªôt k√≠ch th∆∞·ªõc sang Integer/Double.
    2. ƒê·ªìng b·ªô x√≥a t·ª´ MySQL.
    """

    # Logic Transform ƒë·∫∑c th√π c·ªßa Product
    def transform_logic(df):
        # 1. X√≥a d·ªØ li·ªáu r√°c
        df = df.na.drop()

        # 2. √âp ki·ªÉu d·ªØ li·ªáu (Cast Types) nh∆∞ code c≈© c·ªßa b·∫°n
        # L∆∞u √Ω: Ki·ªÉm tra t√™n c·ªôt k·ªπ l∆∞·ª°ng
        cols_to_int = [
            "product_description_length",
            "product_photos_qty",
            "product_length_cm",
            "product_height_cm",
            "product_width_cm",
        ]

        for c in cols_to_int:
            # Ch·ªâ cast n·∫øu c·ªôt ƒë√≥ t·ªìn t·∫°i trong DF
            if c in df.columns:
                df = df.withColumn(c, col(c).cast("integer"))

        # V√≠ d·ª•: product_weight_g c≈©ng n√™n l√† int ho·∫∑c double
        if "product_weight_g" in df.columns:
            df = df.withColumn(
                "product_weight_g", col("product_weight_g").cast("integer")
            )

        return df

    return _process_silver_asset(
        context,
        df=silver_stg_product,
        target_table="silver.clean_product",
        asset_key="clean_product",
        merge_key="product_id",
        transform_func=transform_logic,
        # --- C·∫§U H√åNH SYNC DELETE ---
        mysql_table="products",  # T√™n b·∫£ng g·ªëc trong MySQL
        mysql_key="product_id",  # T√™n c·ªôt ID trong MySQL
        logger=context.log,  # Logger
    )


# ==============================================================================
# 4. ORDER ITEMS (S·ª≠ d·ª•ng Single Key: order_item_id)
# ==============================================================================
@asset(
    description="Clean & Deduplicate Order Items",
    ins={"silver_stg_order_item": AssetIn(key_prefix=["silver", "orderitem"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "orderitem"],
    name="silver_cleaned_order_item",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "order_item_id"},
)
def silver_cleaned_order_item(context, silver_stg_order_item: DataFrame):

    def transform_logic(df):
        # Cast v√† Round s·ªë li·ªáu
        df = df.withColumn("price", round(col("price"), 2).cast("double")).withColumn(
            "freight_value", round(col("freight_value"), 2).cast("double")
        )

        # Kh√¥ng c·∫ßn t·∫°o c·ªôt composite key n·ªØa
        return df.na.drop()

    return _process_silver_asset(
        context,
        df=silver_stg_order_item,
        target_table="silver.clean_order_item",
        asset_key="clean_order_item",
        # Merge Key l√† order_item_id
        merge_key="order_item_id",
        transform_func=transform_logic,
        # --- C·∫§U H√åNH SYNC DELETE ---
        mysql_table="order_items",
        mysql_key="order_item_id",  # L·∫•y tr·ª±c ti·∫øp c·ªôt n√†y l√†m m·ªëc so s√°nh
        logger=context.log,
    )


# ==============================================================================
# 5. PAYMENTS (MD5 Key Strategy)
# ==============================================================================
@asset(
    description="Clean Payments with MD5 Composite Key",
    ins={"silver_stg_payment": AssetIn(key_prefix=["silver", "payment"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "payment"],
    name="silver_cleaned_payment",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "pk_hash"},
)
def silver_cleaned_payment(context, silver_stg_payment: DataFrame):

    def transform_logic(df):
        df = df.withColumn(
            "payment_value", round(col("payment_value"), 2).cast("double")
        )
        if "payment_installments" in df.columns:
            df = df.withColumn(
                "payment_installments", col("payment_installments").cast("integer")
            )

        # --- T·∫†O MD5 KEY ---
        # Logic: MD5(order_id + payment_sequential)
        df = df.withColumn(
            "pk_hash", md5(concat(col("order_id"), col("payment_sequential")))
        )
        return df.na.drop()

    return _process_silver_asset(
        context,
        df=silver_stg_payment,
        target_table="silver.clean_payment",
        asset_key="clean_payment",
        merge_key="pk_hash",
        transform_func=transform_logic,
        # --- MYSQL CONFIG ---
        mysql_table="order_payments",
        mysql_key="MD5(CONCAT(order_id, payment_sequential))",
        logger=context.log,
    )


# ==============================================================================
# 6. ORDER REVIEWS
# ==============================================================================
@asset(
    description="Clean & Sync Delete for Order Reviews",
    ins={"silver_stg_order_review": AssetIn(key_prefix=["silver", "orderreview"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "orderreview"],
    name="silver_cleaned_order_review",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "review_id"},
)
def silver_cleaned_order_review(context, silver_stg_order_review: DataFrame):

    def transform_logic(df):
        # Drop c·ªôt title n·∫øu c·∫ßn thi·∫øt nh∆∞ logic c≈©
        if "review_comment_title" in df.columns:
            df = df.drop("review_comment_title")
        return df.na.drop()

    return _process_silver_asset(
        context,
        df=silver_stg_order_review,
        target_table="silver.clean_order_review",
        asset_key="clean_order_review",
        merge_key="review_id",
        transform_func=transform_logic,
        # C·∫•u h√¨nh Sync Delete
        mysql_table="order_reviews",
        mysql_key="review_id",
        logger=context.log,
    )


# ==============================================================================
# 7. ORDERS
# ==============================================================================
@asset(
    description="Clean & Sync Delete for Orders",
    ins={"silver_stg_order": AssetIn(key_prefix=["silver", "order"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "order"],
    name="silver_cleaned_order",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "order_id"},
)
def silver_cleaned_order(context, silver_stg_order: DataFrame):

    return _process_silver_asset(
        context,
        df=silver_stg_order,
        target_table="silver.clean_order",
        asset_key="clean_order",
        merge_key="order_id",
        transform_func=lambda df: df.na.drop(),
        # C·∫•u h√¨nh Sync Delete
        mysql_table="orders",
        mysql_key="order_id",
        logger=context.log,
    )


# ==============================================================================
# 8. PRODUCT CATEGORY
# ==============================================================================
@asset(
    description="Clean Product Categories",
    ins={
        "silver_stg_product_category": AssetIn(key_prefix=["silver", "productcategory"])
    },
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "productcategory"],
    name="silver_cleaned_product_category",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "product_category_name"},
)
def silver_cleaned_product_category(context, silver_stg_product_category: DataFrame):

    return _process_silver_asset(
        context,
        df=silver_stg_product_category,
        target_table="silver.clean_product_category",
        asset_key="clean_product_category",
        merge_key="product_category_name",
        transform_func=lambda df: df.na.drop(),
        # B·∫£ng n√†y th∆∞·ªùng l√† Static ho·∫∑c Translation, c√≥ th·ªÉ kh√¥ng c·∫ßn sync delete
        # N·∫øu mu·ªën sync: mysql_table="product_category_name_translation"
        mysql_table=None,
        mysql_key=None,
        logger=context.log,
    )


# ==============================================================================
# 9. GEOLOCATION
# ==============================================================================
@asset(
    description="Clean Geolocation Data (Filter Brazil Bounds)",
    ins={"silver_stg_geolocation": AssetIn(key_prefix=["silver", "geolocation"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "geolocation"],
    name="silver_cleaned_geolocation",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "geolocation_zip_code_prefix"},
)
def silver_cleaned_geolocation(context, silver_stg_geolocation: DataFrame):

    def transform_logic(df):
        df = df.na.drop()
        # Filter t·ªça ƒë·ªô Brazil
        df = df.filter(
            (col("geolocation_lat") <= 5.27438888)
            & (col("geolocation_lng") >= -73.98283055)
            & (col("geolocation_lat") >= -33.75116944)
            & (col("geolocation_lng") <= -34.79314722)
        )
        return df

    return _process_silver_asset(
        context,
        df=silver_stg_geolocation,
        target_table="silver.clean_geolocation",
        asset_key="clean_geolocation",
        # Geo data tr√πng l·∫∑p r·∫•t nhi·ªÅu, dedup theo zip code prefix
        merge_key="geolocation_zip_code_prefix",
        transform_func=transform_logic,
        # Geo data th∆∞·ªùng append-only ho·∫∑c static, √≠t khi delete t·ª´ng d√≤ng
        mysql_table="geolocation",
        mysql_key="geolocation_zip_code_prefix",
        logger=context.log,
    )


# ==============================================================================
# 10. DATE DIMENSION (Derived from Orders)
# ==============================================================================
@asset(
    description="Extract Date Dimension from Orders",
    # D√πng Staging Order l√†m input ƒë·ªÉ l·∫•y c√°c ng√†y m·ªõi ph√°t sinh
    ins={"silver_stg_order": AssetIn(key_prefix=["silver", "order"])},
    io_manager_key="spark_io_manager",
    key_prefix=["silver", "date"],
    name="silver_date",
    compute_kind=COMPUTE_KIND,
    group_name=LAYER,
    metadata={"merge_key": "order_purchase_timestamp"},
)
def silver_date(context, silver_stg_order: DataFrame):
    """
    T·∫°o b·∫£ng Date Dimension t·ª´ Order Purchase Timestamp.
    Kh√¥ng c·∫ßn Sync Delete v√¨ ƒë√¢y l√† b·∫£ng Dimension d·∫´n xu·∫•t.
    """

    def transform_logic(df):
        # Ch·ªâ l·∫•y c·ªôt timestamp distinct
        return df.select("order_purchase_timestamp").na.drop().distinct()

    return _process_silver_asset(
        context,
        df=silver_stg_order,
        target_table="silver.date_dimension",
        asset_key="date_dimension",
        merge_key="order_purchase_timestamp",
        transform_func=transform_logic,
        # Kh√¥ng sync delete
        mysql_table=None,
        mysql_key=None,
        logger=context.log,
    )
