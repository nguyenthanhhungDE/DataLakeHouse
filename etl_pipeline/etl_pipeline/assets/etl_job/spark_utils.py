from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, max as spark_max, current_timestamp, lit


from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, lit, current_timestamp
from delta.tables import DeltaTable


def get_watermark_from_meta(
    spark: SparkSession,
    meta_table: str,
    asset_key: str,
    default_value="1900-01-01 00:00:00",
):
    """
    L·∫•y watermark. N·∫øu ch∆∞a c√≥ trong DB (l·∫ßn ƒë·∫ßu ch·∫°y), tr·∫£ v·ªÅ default_value (Low Watermark).
    """
    try:
        # 1. ƒê·∫£m b·∫£o b·∫£ng Metadata t·ªìn t·∫°i (Idempotent)
        spark.sql(
            f"""
            CREATE TABLE IF NOT EXISTS {meta_table} (
                asset_key STRING,
                watermark_value TIMESTAMP,
                last_updated TIMESTAMP
            ) USING DELTA
        """
        )

        # 2. Query l·∫•y watermark c·ªßa asset_key c·ª• th·ªÉ
        df = spark.sql(
            f"SELECT watermark_value FROM {meta_table} WHERE asset_key = '{asset_key}'"
        )
        rows = df.collect()

        # 3. Check k·∫øt qu·∫£
        if rows and rows[0]["watermark_value"]:
            # N·∫øu ƒë√£ c√≥ watermark -> Tr·∫£ v·ªÅ gi√° tr·ªã Timestamp t·ª´ DB
            return rows[0]["watermark_value"]

        # 4. N·∫øu KH√îNG t√¨m th·∫•y (Empty list) -> Tr·∫£ v·ªÅ Default (1900)
        print(
            f"‚ÑπÔ∏è No watermark record found for '{asset_key}'. Using default: {default_value}"
        )
        return default_value

    except Exception as e:
        # N·∫øu l·ªói (v√≠ d·ª• ch∆∞a c√≥ quy·ªÅn t·∫°o b·∫£ng) -> V·∫´n tr·∫£ v·ªÅ Default ƒë·ªÉ job ch·∫°y ti·∫øp (Full Load)
        print(f"‚ö†Ô∏è Error reading metadata table: {e}. Defaulting to {default_value}")
        return default_value


def update_watermark_meta(
    spark: SparkSession, meta_table: str, asset_key: str, new_watermark
):
    """
    C·∫≠p nh·∫≠t (Upsert) watermark m·ªõi v√†o b·∫£ng Metadata sau khi x·ª≠ l√Ω xong.
    """
    if not new_watermark:
        return

    # T·∫°o DataFrame ch·ª©a th√¥ng tin m·ªõi
    new_data = [(asset_key, new_watermark)]
    df = spark.createDataFrame(new_data, ["asset_key", "watermark_value"]).withColumn(
        "last_updated", current_timestamp()
    )

    # Upsert v√†o b·∫£ng Metadata (D√πng Merge)
    from delta.tables import DeltaTable

    if DeltaTable.isDeltaTable(spark, meta_table):
        delta_table = DeltaTable.forName(spark, meta_table)
        (
            delta_table.alias("t")
            .merge(df.alias("s"), "t.asset_key = s.asset_key")
            .whenMatchedUpdateAll()
            .whenNotMatchedInsertAll()
            .execute()
        )
    else:
        # Fallback t·∫°o m·ªõi n·∫øu ch∆∞a l√† Delta Table (l·∫ßn ƒë·∫ßu)
        df.write.format("delta").mode("append").saveAsTable(meta_table)


def process_dedup_logic(
    spark: SparkSession,
    df: DataFrame,
    merge_key: str,
    watermark_col: str,
    high_watermark=None,
):
    """
    Logic chung: Filter Data M·ªõi -> Deduplicate l·∫•y d√≤ng m·ªõi nh·∫•t.
    """
    input_df = df

    # 1. Filter Incremental
    if high_watermark:
        input_df = df.filter(col(watermark_col) > high_watermark)

    if input_df.rdd.isEmpty():
        return input_df, None  # Kh√¥ng c√≥ data m·ªõi

    # 2. T√≠nh to√°n Watermark m·ªõi (Max c·ªßa batch hi·ªán t·∫°i)
    # ƒê·ªÉ c·∫≠p nh·∫≠t v√†o metadata table sau n√†y
    new_max_watermark = input_df.agg(spark_max(watermark_col)).collect()[0][0]

    # 3. Deduplicate Logic (SQL Style)
    view_name = f"v_temp_{merge_key}"
    input_df.createOrReplaceTempView(view_name)

    dedup_sql = f"""
        SELECT * FROM (
            SELECT *,
            ROW_NUMBER() OVER (PARTITION BY {merge_key} ORDER BY {watermark_col} DESC) as rn
            FROM {view_name}
        ) t WHERE rn = 1
    """
    deduped_df = spark.sql(dedup_sql).drop("rn")

    return deduped_df, new_max_watermark


def calculate_merge_metrics(
    spark: SparkSession, source_df: DataFrame, target_table: str, merge_key: str
):
    """
    T√≠nh to√°n tr∆∞·ªõc s·ªë d√≤ng Insert/Update.
    """
    try:
        if not spark.catalog.tableExists(target_table):
            return source_df.count(), 0  # Table ch∆∞a c√≥ -> To√†n b·ªô l√† Insert

        source_count = source_df.count()
        if source_count == 0:
            return 0, 0

        # T·∫°o view cho source
        source_df.createOrReplaceTempView("v_metrics_source")

        # ƒê·∫øm Update (Inner Join)
        sql = f"""
            SELECT COUNT(1) as cnt 
            FROM v_metrics_source s
            INNER JOIN {target_table} t ON s.{merge_key} = t.{merge_key}
        """
        updates = spark.sql(sql).collect()[0]["cnt"]
        inserts = source_count - updates

        return inserts, updates
    except Exception as e:
        print(f"Metrics Error: {e}")
        return -1, -1


# ==============================================================================
# H√ÄM 1: GET SOURCE KEYS (C√ì LOGGER)
# ==============================================================================
def get_source_keys_jdbc(
    spark: SparkSession,
    config: dict,
    target_col_name: str = None,
    logger=None,
):
    """
    K·∫øt n·ªëi MySQL qua JDBC v√† l·∫•y danh s√°ch Key.
    """

    # Helper ƒë·ªÉ log: N·∫øu c√≥ logger th√¨ d√πng logger, kh√¥ng th√¨ print
    def log_msg(msg):
        if logger:
            logger.info(msg)
        else:
            print(msg)

    try:
        # 1. L·∫•y th√¥ng tin c·∫•u h√¨nh
        jdbc_url = config.get("url")
        user = config.get("user")
        password = config.get("password")
        table_name = config.get("table")
        source_key_column = config.get("key")
        driver = config.get("driver", "com.mysql.cj.jdbc.Driver")

        # 2. Query t·ªëi ∆∞u
        query = f"(SELECT {source_key_column} FROM {table_name}) AS t"

        log_msg(f"üîå JDBC Connecting to {table_name}...")

        # Ch·ªâ print user/url ra console ƒë·ªÉ debug, h·∫°n ch·∫ø log pass l√™n UI
        if not logger:
            print(f"   - URL: {jdbc_url}")

        # 3. ƒê·ªçc d·ªØ li·ªáu
        df = (
            spark.read.format("jdbc")
            .option("url", jdbc_url)
            .option("dbtable", query)
            .option("user", user)
            .option("password", password)
            .option("driver", driver)
            .option("fetchsize", "10000")
            .load()
        )

        # 4. ƒê·ªïi t√™n c·ªôt
        final_col_name = target_col_name if target_col_name else source_key_column
        if source_key_column != final_col_name:
            log_msg(f"   - Renaming column '{source_key_column}' -> '{final_col_name}'")
            df = df.withColumnRenamed(source_key_column, final_col_name)

        return df

    except Exception as e:
        error_msg = f"‚ùå CRITICAL ERROR reading JDBC: {e}"
        if logger:
            logger.error(error_msg)
        else:
            print(error_msg)
        raise e


# ==============================================================================
# H√ÄM 2: SYNC DELETED RECORDS (C√ì LOGGER & SCHEMA STRING)
# ==============================================================================
def sync_deleted_records(
    spark: SparkSession,
    target_table: str,
    full_source_df: DataFrame,
    merge_key: str,
    logger=None,
):
    """
    ƒê·ªìng b·ªô d·ªØ li·ªáu x√≥a.
    T√çNH NƒÇNG M·ªöI: T·ª± ƒë·ªông th√™m c·ªôt 'is_active' n·∫øu b·∫£ng c≈© ch∆∞a c√≥.
    """

    def log_info(msg):
        if logger:
            logger.info(msg)
        else:
            print(msg)

    def log_error(msg):
        if logger:
            logger.error(msg)
        else:
            print(f"ERROR: {msg}")

    try:
        # 1. KI·ªÇM TRA B·∫¢NG C√ì T·ªíN T·∫†I KH√îNG (D√πng SQL Show Tables)
        table_exists = False
        if "." in target_table:
            db_name, table_name = target_table.split(".", 1)
            db_name = db_name.replace("`", "")
            table_name = table_name.replace("`", "")
            count = spark.sql(f"SHOW TABLES IN {db_name} LIKE '{table_name}'").count()
            table_exists = count > 0
        else:
            count = spark.sql(f"SHOW TABLES LIKE '{target_table}'").count()
            table_exists = count > 0

        if not table_exists:
            log_info(
                f"‚ÑπÔ∏è Target table '{target_table}' does not exist. Skip delete sync."
            )
            return 0

        # ======================================================================
        # 2. AUTO-MIGRATION: T·ª∞ ƒê·ªòNG TH√äM C·ªòT IS_ACTIVE N·∫æU THI·∫æU
        # ======================================================================
        # L·∫•y danh s√°ch c·ªôt hi·ªán t·∫°i c·ªßa b·∫£ng ƒë√≠ch
        current_cols = spark.read.table(target_table).columns

        if "is_active" not in current_cols:
            log_info(
                f"‚ö†Ô∏è Column 'is_active' missing in {target_table}. Starting Auto-Migration..."
            )

            # B∆∞·ªõc A: Th√™m c·ªôt is_active v√†o c·∫•u tr√∫c b·∫£ng (Delta Lake)
            log_info(
                f"üî® Executing: ALTER TABLE {target_table} ADD COLUMNS (is_active BOOLEAN)"
            )
            spark.sql(f"ALTER TABLE {target_table} ADD COLUMNS (is_active BOOLEAN)")

            # B∆∞·ªõc B: C·∫≠p nh·∫≠t d·ªØ li·ªáu c≈© -> Set is_active = true (M·∫∑c ƒë·ªãnh)
            # L∆∞u √Ω: L·ªánh UPDATE n√†y c√≥ th·ªÉ t·ªën th·ªùi gian n·∫øu b·∫£ng r·∫•t l·ªõn
            log_info(f"üî® Executing: UPDATE {target_table} SET is_active = true")
            spark.sql(f"UPDATE {target_table} SET is_active = true")

            log_info(f"‚úÖ Auto-Migration completed. Table schema updated.")
        # ======================================================================

        # 3. L·∫§Y D·ªÆ LI·ªÜU TARGET (CH·ªà L·∫§Y ACTIVE)
        # L√∫c n√†y ch·∫Øc ch·∫Øn ƒë√£ c√≥ c·ªôt is_active n√™n kh√¥ng b·ªã l·ªói n·ªØa
        target_df = (
            spark.read.table(target_table).filter("is_active = true").select(merge_key)
        )

        # In th√¥ng tin debug l√™n UI
        target_schema_str = target_df._jdf.schema().treeString()
        target_count = target_df.count()

        log_info(
            f"üìã SILVER TARGET ({target_table}):\n"
            f"   - Active Keys: {target_count}\n"
            f"   - Schema:\n{target_schema_str}"
        )

        # 4. L·∫§Y D·ªÆ LI·ªÜU SOURCE
        source_keys = full_source_df.select(merge_key).distinct()
        source_count = source_keys.count()
        log_info(f"üìä MySQL Source Keys: {source_count}")

        # 5. T√åM ID C·∫¶N X√ìA (Anti Join)
        ids_to_delete = target_df.join(source_keys, on=merge_key, how="left_anti")
        delete_count = ids_to_delete.count()

        if delete_count == 0:
            log_info(
                f"‚úÖ No records to delete via Anti-Join.\n"
                f"   (Source={source_count}, Target={target_count})"
            )
            return 0

        log_info(
            f"üóë DETECTED: Found {delete_count} records to Soft Delete in {target_table}..."
        )

        # 6. TH·ª∞C HI·ªÜN UPDATE (MERGE DELETE)
        delta_table = DeltaTable.forName(spark, target_table)

        (
            delta_table.alias("t")
            .merge(ids_to_delete.alias("s"), f"t.{merge_key} = s.{merge_key}")
            .whenMatchedUpdate(
                set={"is_active": "false", "_ingested_at": "current_timestamp()"}
            )
            .execute()
        )

        log_info(f"‚úÖ Soft Delete committed successfully.")
        return delete_count

    except Exception as e:
        log_error(f"‚ö†Ô∏è Error syncing deletes: {e}")
        # Return -1 ƒë·ªÉ bi·∫øt l√† c√≥ l·ªói nh∆∞ng kh√¥ng l√†m crash pipeline
        return -1
